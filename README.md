# üå¨Ô∏è Co-piloto E√≥lico: Pipeline MLOps para An√°lise de Turbinas

## Sobre o Projeto

O **Co-piloto E√≥lico** √© um sistema de ponta a ponta que utiliza Machine Learning e IA Generativa para monitorar a sa√∫de e prever a performance de turbinas e√≥licas a partir de dados SCADA.

O projeto implementa um pipeline MLOps completo, desde o processamento dos dados e treinamento de modelos at√© o deploy via API e a intera√ß√£o atrav√©s de um assistente de IA conversacional inteligente.

## ‚ú® Principais Funcionalidades

* **Detec√ß√£o de Anomalias:** Utiliza um modelo **LSTM Autoencoder** treinado em PyTorch para identificar comportamentos an√¥malos na opera√ß√£o da turbina, analisando o erro de reconstru√ß√£o dos dados.
* **Previs√£o de Gera√ß√£o de Pot√™ncia:** Emprega um modelo **XGBoost** para prever a gera√ß√£o de energia, permitindo um planejamento mais eficaz da opera√ß√£o e manuten√ß√£o.
* **Interface de IA Conversacional:** Permite que operadores consultem o status da turbina em linguagem natural atrav√©s do **"Co-piloto E√≥lico"**, uma interface constru√≠da com a API do Google Gemini.
* **Ciclo de Vida MLOps:** Gerencia todo o ciclo de vida dos modelos com **MLflow**, incluindo o registro de experimentos, artefatos (como scalers) e o versionamento dos modelos.
* **API de Infer√™ncia:** Disponibiliza os modelos treinados atrav√©s de uma **API FastAPI** robusta e otimizada, que serve como o "c√©rebro" para o Co-piloto.

## üõ†Ô∏è Stack Tecnol√≥gica

* **An√°lise e Modelagem:** Python, Pandas, Scikit-learn, PyTorch, XGBoost
* **MLOps:** MLflow
* **API:** FastAPI, Uvicorn
* **IA Generativa:** Google Gemini
* **Orquestra√ß√£o:** Scripts Python para pipelines de dados e treinamento.

## üöÄ Como Rodar o Sistema Completo

Para executar o sistema, voc√™ precisar√° de 3 terminais rodando simultaneamente.

### Pr√©-requisitos
- Python 3.10+
- Todas as bibliotecas instaladas a partir de um arquivo `requirements.txt` (sugest√£o).
- Dados brutos (`Aventa_AV7_IET_OST_SCADA.csv`) na pasta `Data/`.

### Etapa 1: Preparar os Dados e Treinar os Modelos

Se esta for a primeira execu√ß√£o, processe os dados e treine os modelos.

```bash
# 1. Processar os dados
python main.py process_data

# 2. Treinar o modelo de detec√ß√£o de falhas (LSTM Autoencoder)
python main.py train fault_detection 3LSTM_Autoencoder

# 3. Treinar o modelo de previs√£o de pot√™ncia (XGBoost)
python main.py train power_forecasting 2XGboosting
```

### Etapa 2: Executar os Servi√ßos em Produ√ß√£o

**Terminal 1 - Servidor de Modelos (MLflow):**
Este terminal serve o banco de dados de modelos que a API ir√° consultar.
```bash
mlflow ui
```

**Terminal 2 - Servidor da API:**
Este terminal carrega os modelos do MLflow e os exp√µe atrav√©s de endpoints.
```bash
uvicorn inference_api:app --reload
```

**Terminal 3 - Co-piloto E√≥lico (Cliente):**
Este √© o terminal com o qual voc√™ ir√° interagir.
```bash
python co_piloto.py
```

Ap√≥s iniciar, fa√ßa perguntas como: `Qual o status da turbina para o dia 2022-02-07?`

## Arquitetura do Sistema

O fluxo de informa√ß√£o ocorre da seguinte maneira:

1.  O **Usu√°rio** faz uma pergunta em linguagem natural no script `co_piloto.py`.
2.  O **Google Gemini** interpreta a pergunta e aciona a ferramenta `gerar_relatorio_diario`.
3.  A ferramenta faz uma requisi√ß√£o HTTP para a **API FastAPI** (`inference_api.py`).
4.  A API utiliza os especialistas (`AnomalyAnalyzer` e `Forecaster`), que carregam os **modelos do MLflow** para processar a solicita√ß√£o.
5.  A API retorna um relat√≥rio estruturado (JSON) para o Co-piloto.
6.  O **Google Gemini** recebe o JSON e o traduz em uma resposta formatada e inteligente para o usu√°rio, seguindo as regras de neg√≥cio definidas no prompt.
